{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCtGQmRcDOQV"
      },
      "source": [
        "#Breve descripción del proyecto:\n",
        "\n",
        "Escogimos el juego de Centipede de atari, la versión 4 ya que la versión 5 no podía correr en colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "nXoOY4sEYKu8",
        "outputId": "5e9ea1ee-a2fc-4663-d630-7aaab6a17320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install gym[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOFpsRw7XoZO",
        "outputId": "7a6247bf-9b83-4564-b754-6ecd1da2eec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gym[accept-rom-license])\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.65.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license])\n",
            "  Using cached AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=acb3e66f4101ce1e42a39243607cf4eaa3cd6e359ff4d4fc3dcb541f72fb714e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG_WNu4jGZ6b",
        "outputId": "99927c0e-3d49-485d-860a-57c0e6dd23af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('Centipede-v4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aO91YhZYXTza"
      },
      "outputs": [],
      "source": [
        "#!pip install pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnCOWRvSCBGC"
      },
      "source": [
        "###Primer dificultad con la que nos encontramos:\n",
        "El vector que de estados que nos regresa el juego es un vector R3 y entonces el problema se vuelve complejo, ergo necesitamos varias capas.\n",
        "\n",
        "#### Arquitectura:\n",
        "Capa Inicial ----> función de activación RELU ----> otra capa lineal ---> RELU y finalmente la capa de salida.\n",
        "¿Por qué? cuando solo teniamos una capa oculta, el puntaje era muy bajo siempre, y decidimos por implementar otra capa para que la transformación ayudara a encontrar una función objetivo \"buena\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ_eBBfBkkY8"
      },
      "source": [
        "![Captura de pantalla 2023-05-18 143441.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABCcAAACECAYAAACj8kH3AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAACTDSURBVHhe7d0NdFTnfefxn4vXkyUnyrJHpPSgHPcwG+lkYqvMGh0rVeJZi7UatNHaKqotW7XlKKBEDhSKZatGNcZQbMumjlxilKVEMTGySSyKHXFwImriccJ6ODYeystQEoZjH4aGRHNKmTSE8UJnnzv3jjR6Gb1g5BmZ7+ecYea+zL3Pfe5zL/P89dznueo3//67hAAAAAAAALLk95x3AAAAAACArCA4AQAAAAAAsorgBAAAAAAAyCqCEwAAAAAAIKsITgAAAAAAgKy6KmE4nwEAAAAAAD50tJwAAAAAAABZRXACAAAAAABkFcEJAAAAAACQVQQnAAAAAABAVhGcAAAAAAAAWUVwAgAAAAAAZBXBCQAAAAAAkFUEJwAAAAAAQFYRnAAAAAAAAFlFcAIAAAAAAGQVwQkAAAAAAJBVBCcAAAAAAEBWEZwAAAAAAABZRXACAAAAAABkFcEJAAAAAACQVQQnAAAAAABAVhGcAAAAAAAAWUVwAgAAAAAAZBXBCQAAAAAAkFUEJwAAAAAAQFYRnAAAAAAAAFlFcAIAAAAAAGQVwQkAAAAAAJBVBCcAAAAAAEBWEZwAAAAAAABZRXACAAAAAABkFcEJAACADCK729X+XEBRZ3oqmIppntriCm9fo9rSIhUVlahySbcizhIAkyu2v0vtj3fIzw3vI4HgBAAAyH3xgNZ4rcpftbpOOPM+BH0HOtSxd6CqGT/QrsqiYjW+kru/hAenOa7gM5Uq8jaq57QzC5dVfH+7Gld2KXjGmoop/Mu4yfWPiNM9ajTXXeUzwY/OMWUS82vNwmp1HnCmL7uoepqKVbSgXcEpnZm5dU95/12/Op4Lqu+CMwNTGsEJAACQ8+L7etV1ziXX9JA6d4ecuUD2hX7WpYh8WtV7TMdCB7X3/1TJ7SzDFHKuT5HDIfVddKYBfOgITgAAgBwXk/+VLmlui1rucinyfb9CWapAuOYu165jB7Xp1nxnTq5zybtsl44FN6lqljMLl1FUkX+OS9f5VHqtmZzmUn5+nr3oo2BWlTYFj2nXMq8pSfhg8lXVcVDHXl0u75TOTO4p4xX/RZeaF1SqzFemxs2hYa2PojuXqsS7VD08ktKP4AQAAMhtZ/zq2Sl5b/OprrxOrlPd8h91lqU7E1bwQCT5AzB2IqjA3oACh8300EDGxbgihwMKW83w4xGFrPX2Bu1p5I79bSoqalR3jj+OEreak/9Xlz5pT05NOZjXwaeKVLS4+7L3nTJZ28WH5GJM4f3WPdt6hXT83KU9IzPp5cD8P9X8Lalp+y698hefk/+ppeoc9P9WVK+/0quY1SLwGmcWCE4AAIDcFtvbq175VHNzgTS3Sk2zI+p4NegsHRDcXKnatZ1qX1aikgW1qm+oV/3C+SpZ2Da4s7S+Hq1ZWK/2Z9tUXTpf1dZ6DbWqLC3R0hfDoz9bH+1V621lat/vTKdEA+p8sFZlyX4xisz+l6pj3+CfvdF9HVq6oCS5vKi0Ukuf6lEo5iwcSSykHvODtjLZ0aK9zbY9E+9qMfqjVlX72jWQY0F1LjT7327SdzqgjqYyFZvtF/sah6U56Vx4UDqKfbVas31wPkX3dqr5Lns7Rd4y1T7YpeCgYxvYZ/xwpxp9xWZbJap9OqCRsiB2tFuta7vMJ79affZ+i4ranGMYY1vjyLf+PDEVnVBaZ5a1q7sVPueslOR0dpnch33szZsDiloBr+R3v6nON8znN1pVlkyjeaVXeKz8XVLtlItild1mysXeIXmcVqYiP3LSYvKwcbud5oG0RhXY2Ghvq9Scgx85x2TKXseSSpVY+06fn2LSMFA2TRrualbXgYFcH29eD2LyuHt1fVoe15syEVJsSCAwuLlalctMfqSn3Tq2jU4ejiSZr61a84L5nJ6vT6Vd8855q0+7nurNuRv1ehrPdh2xA11qvsPedskda9T9i+F3hei+tDKf7Ai1Q4ERLp90/fnhTKefe+s8rBlrn+nXmbNP/3uD14vuNfcZs83UtVhtpWtI0OmDlqkPdE9J7nOp+X7qmqo298JOdT7erPrbaodU4AeLvmHu2fPMcd9l3bOtV7Xq/ybgLB0w6v1orHJgLd/ZNuRe7VdkQq31Yup9ukely+rknh7XkUN+Z36a2DsKWPeO67xyj9nYKqqeZU46L+k1dVpnEJwAAAA5LKKe53ulL1WpwmpCPM0jX02B4i/0KDBSFOGwqaDetEV7g/bz/6911KngqKnAPjT8L2T+kEstL7+lg6FjOrh3h1aVx9W7utFUFEbasONCTH1HTaXYmUw61a3GW+rV9kpQUadiGzvRq/Z77lH7AXtNqyPNe+5pV+8J5xfymbB6Nzer21k+TLRHS82P9ubNvf0tOqxtdjaZ/YyWvpH8tk+h04O/03c4rNjPv6NGU6ls32MfT/y036R5ibres9dJOhdUm6l0pKcjfjqorpWmQuF0ChrcWKKyhjb17Hfy5VxUwVdMJfv+wXlu7TP+q5fUeneb/Mn0mErANJeG/y43FZ27WtV9NPNxZtzWePMtmSd9pgJRq+q0ziyDL5pK19OB/vMbfcVUoqzlTv5Zx97zVJt+bOXRoU7VruxWOLkkTaFb1kM/8f1tqvSZ/N0dcspFXNGjplw0lKt6Y1oHk6ky9XaHGpc5aTF5qKs/Zi9PpjVkKovlqn/Gb2/rjDkHy0zFb6dd9tp3m/NprZuan6rgHehQiUnDQNk0adjfYyrBpuwlK6zjzGvnc5JV3k0et77otD4yYicCyTIxf0XP4OvsTMisE9Z30tNujs3/TL2W/GBIECXFydeQcy0NczGi7iVfSJ6XQNr1FLDOXYWphGVq/THWdh1hU3GtvWONepwAjhWoaL29bdD9JrK9UeX3pJV5k/vh3e2qv9tU2DNnpZ0f6ft3zn3fnjbV3tbaHzQavk+rE8pqladfZ84+G//Ob597a52nKs21aO4zqXuUyeuQla4F1epIv9d8kDJlueR7iqm0P2jts7f/+3GTjt7NbWp7ztzTjwbVl+H8JO+hizsVX7BOOwLm/n7MvAI7tO7LBc4atjHvR6OWg6h6VpiydX/nkHt1o+qfHrhmw89Vq7i0Vb1O+R8mFlBvpEK+OeZzPCj/K+Z9do18hcmlNnOsPebNdZNnHH3U5KvqGeeYL+m1QVVT5UnEBAAAQK56d2vijsLCxJKes84M49DGRHnh9YlHf3bemWF758nCROGilxJ9znTK8e/dkSgsLE9sDDkzfvlSYrHZ5hNvO9Mpv3098ehcs40VP06k9jZsm8O+ezKx9fbCRPmKlxJH+gbScz7yw8QSs17hk+8kp99pvz5R+MePJl5PbfjC+URf+Eji5OBDSHM88dKTWxPvvHs2cf6CPefsz55ILDDbvL7d3mYmQ9Pc173YHP8TiYFvvZN4wtrOrSsTLx3qs7dv0nP8BWu9wsTi7tQ3zyfeXGvSffsTiTffTUvov76Z+OafDuzj7GsbExtfO57o+629OHHepP3r5nuFixM/7M84e5+F5rzd8e13EmedfZ49mzEDRkh3ymjbGl++2duel7jn2TcTJ1Pp/tfXE0/cbG03tc++xA+/bqbv3po47mwrcf5s4uSh4/3lw1rnpUUDedHvwpHERmtbcxcntqby2Dj/riljX7L2cUdi67v2vFSZKixckFjZc9w5H2cTZ5102Wk1y154p7+8nA9vtb8z97bEE+Y7yTwwTn5/ceJ6M7//HJ59PbHx2dcTx3/pfNE6z6l1Xh5I8Vh5PVAmziZ++Bd2Wh997eTAcfUdSWxNnvPB12WyLJo0ruw+kkhdHv1pH+FaHeDkq3P9pDvbs8TspzCxYO3rA+fufF/iyPfs47p+7Zum5GaSebup8zDvKxsHyrs5D68/Vp7cX/81n7wnlSfu//7AMVn5evJlO13D7itpMt1PxtxnaKNdhu/85qBr8fwvjyfe+blTGpP3RbPO17cO3IusdL32aPK7hbdvNXcr2wcqU8bw8jLOe0rfD5P7KH/kxwP3vvMnEz9+xBzvHz+ReL1v4LodzDlvN29MHBmy3E7L4sRLv7Snx3c/ylwOjnc/kdj69slE/63J3O+esK7Zud/sP97j370tcf2NKxM//ldnxijO+x9N5kH5t484c2xHvm2f40f9A+cTiQQtJwAAQM4Kv9GtoCpUUZb29/XrfKqZHVfX7oG/cI/GXVYljyIKhgb9TXe46aXy3WredwaH/zU8kxN+dR+QIjtbVV1mN1O2XsXlzep1VrHkfbxAinZrZcsadW63/+KcP8ejgowd47lV80CdvNfmyTXNnpNXVqeGG6X4P0eGtQK5FKV3/6Vqrsu3tz/NJffNFfLZi2zxgHqfNzl8oFP1FQPHVlRar47DzjpGXnmTmsrdyp/uzHCZtN9bZz6Y4xzyx/GCezdpy9e8ynP2mZd36T0DjrytieRbjZbfV6qCVLpn+FSxwPmcdI1cM8w297Wr9aE2de0MKXIxTwXXuUdo7THEz025OCVVrF2vulQeG65rfVr1dy2mPAblPzA4Nb61m7Tuy27nfOQpL5WuJFPm7/T2lxfXnD9RxU3mw7w6fdV8J5kHRsGtdeao0uT51HSfT+5Zzhet87ywQcmz8+5Je95ExALy/8jsZ9l6rSovGDiufI/q/naD2Xdc3W8PGU3HpPEvF3qUPzTtlySmwE/MlTV7udY/5Bs4d658ee7eoA0LzXk219cHGc+n5htNKr02lV958lVU2J8d9j0pop6Hq1VW7FwTnmLNfzD9ip+YsfYZMvsMy6uWx5cPrGe4Zrnl/YxdGkN7u02qKrTusTp5UpltzndB+SpteMhjrmO/goOK3CWWqVGMeU9xuK4eOIZ+0T7Frxm4bgdxHoEouMMnz0jL00zkfjQS98IW1d1QoP5b04xS1X2lVDpnrn8n/9z37tDBwDpVzLCnM4srsMd6ZKpANTeZc9AvqtDbVmLM8RSOkBdXMIITAAAgN10Mqfe7VjWjV83Os+32q1rtpuKnF3vkd1rejmq6SzOdj6MzFdyPOx/H60xk1IqQZ5ZdcXB/ZZt2/G2d/vu7PWpbWa/KsmKVNIzxjHo8ouD2Tq1ZUu08nz1frfvM/AvjCclcBmf6TGVnFLNn6hPOx9gJv7qsZ8YXOs+p39Np5saHdUbqLvxvl23Uh4zbumz5lqeKtXu05QFTvdrXpTX3V2u+t1jVDw/tl2IE52LJvCv4gxHCGJ/xylR1FDt33p52uN2Dm6dfEpep3Dkf+50Jy/9im5obqu0+Ijz1Sp6d9+3FE2KOy7rk3L//KXs63fTPyWsqt/EzsXEFDS/N+4olEzBTnxpWSXXpczeYc2WlcRIvkdjpUa94FYxZYZ24eMwqTV55rBFhMrDXKRhx/+7rkyVOsd/a0xMyUpm6VPkVarrPo8jzjZqfCuwUz9fSFyNyL6qTL1PUL1Xu8kcodyMY7/1oRFaHyfu71bl6qaqdvjvmP2z3a5Hs/HYizP9hAeuRjukVKk1/pCN+RCGrv4nZ5pwy4skgBCcAAEBuOmr/9TmzHvW8MY7ohKlkWy0h8j4+VpfoMfVZz6vPnsCP8Wmu5Lo11hCBIzzru+Nu52niaXnyfLlFG159S8eCe7Wro0nuYLsav52h9ce5gNb45qt+e0zer2/QlpfN947t1bpL/ovzJXD+uul5aNew40q+VvuSxx5+sV5fWNCm0B/WaNWz2/TaQbPshYbkdz+I86aSMGGXO9+m5at00Xpt8x/Uwbde07bVVYr/oFWtr4watjGVkTxTTZQivxyhfP4iKKuqkzfd6VNiMp3oUn15pdoOFajmr/9e23qtcrpNQ8/OuPPaHJdVfwz/6tf2dLpzRxQ0FS7XjMtRmT1v1SVHcI3ykgno06+HVTTjOrLfb6cxYwIybXcCrkle8ckhVodfFztUZ/UzcJm58qzSFFQovT+YIex1IoqM0A9C+FCyxE08+HrZueSZXyHPDI+8N+Qny4lrlld1j+3QthWjDFfrlLtAZOzWPuO7H2UqB3EFHv+C5i/qVmxekzZ891W9FTqmvY+N1P5jHPrCdiDz1tLBLT6OBtVtvd/iVXp7iszoEBMAACCrgq92mJ/aPq3zD60AmNfBLaqbLvXuTnUGl4n5sfmy3dzZd2OmP8s5TvWqe6dUUOMb5w9G47OlqjHp6L6/Xu1704YtNZW96ImQIs4P4Nhhv4Lvxezl0/Pl9nrlnWFSl+mv+Uf96jKVDHd5hSo+W6C8GS7F33tHISvKYr4y+G/ukyTfK99cKfT4UrVuDymaSqo5tth7VkeH1kRUwT1WgMWs+6VSuWdZlYioQoecB2Mm+pfGNH3vWRWqmPqcExwfz5CBlznfwvv8Cp+2z5tV+fPc6EkGHTKet5RC69EjUz4fbhxULuLv+dX2QLtCVn7Nnfwe6qJBvwKmcuS9qUqlc/KTldPoYeexpbRjGHde55Wa82yqwM80J8tEanSOeDSkroeaTYXLpZp54756RtFn8t68nemz92HKXDyZpDyV3lxhrtV2NT/crVCqIh43Ze7FZjVvN+dpoakIOrOHy7Td8fPMqzFH2a2li9oVSB8p41xU4aP2UMaXm6esxpS7oNqWtQ0anSMeDSv4C/uk2ev0qvUbJl2nnHXM8UX2tKn5mZA01ydvtjtFjAfUdne73I9t07YX9uqguZcf9G/TqoWe/sdIRpTnkdfci+Kv9CbLc2bjvR9lKgch+Z83+TnH3D9usVrB5MkVj+id5PfNCk55nzCXHcROCR/wJ8tJlXe818qV0yEmwQkAAJB7zI/YnhfMz7fUKB1DuUpVs9hUE3/Uo9703vnf7tI3Xwwqciam2OlwclSAxuf65HusRVVDmjt3P9sh/4moYmbdyP4utS5qlX9Og9bfO4HKlUlHy+YGuc8F1dEwX8Ue5y9VnmKVLViqnhPWSjEFvtuo2oqSgeWljeo85VbTbaUj/7XwOl8y+BJ6qrr/O8UPhjXzJrP2vlbNH2H4w8uvQHVPr5NveljdKwc/X19SUa2OfValKF+l5V7zbiprqUdvisvVHfea6oHUeU+jMyrExH3aVPCT2y1xjv/LnWP3JXBZ8y2k3ocaVekbOG/FC9bIP92nhlvG6F9/mkcNTw0vF8UV5rwflSmP61U3ShP9yyV/nqmQmvfuZc6wiObclXebqpup6Ok5c204Q4SOP6/zVPXgQJkoSR1XWbXW/Cgm96ItainL+PfvCfi0POXmbftSex8m3ZXP2SnKW9CideWu5Kga1f1lrkzVq3sVM9fvlhUZrqmkzNsdL1dZi7Ysciu+v2NwXyzeMlV+o2f8/dVMxHXmvmTt0xp5KG2fxWWVqu1yRpFIrWOlq9xZxxzf/CbrXPq07um6ZGAtq85EFDkn9f0qLZA7LgWq+UadXKfM8S9qd+7b1uNbbVryN+nDdI73fpSpHHjku9uUnsNtqk7d77zNCv+B1UosoNZye4jdMUfrSJnlVanVkmZ3UCHneK3+iZY+bu/LWzhGwPwKRHACAADknPjbveo+51Ld7RXJ5rwj8fyvBvOD06/un6Q1sT8XUvfqWs0vLVGJr1KNz8VV9dgObVg4/Gd5bG+7GheUqcSsO/+uNer9g+Xa8nyLvKmO1MbJdUOLdvRu0vJbPP2dsOXNKVXd6vX6s+RzxnmqeGSHqThUyO0ESPLmVGj5976n5XMzVKNcpVr1sqnAzrWO3qX88hZt+/smNS2256X6sph0s2u0afc2rbqztD/trlkeVSwzx3uTnYaC2zdp2wM++9hneFX3t7u06msNWpWcV6CZIzwDPx75X16lDXd6nfOfJ7dnZsay0O+y5ptHTS+YyvatXue8mu3dUKd1P9igqnE8J26Vi20mLVYgw96r9f0qtXxvjzaNUB4nxbV12vT9FvmSHWLmyXvneu16uEkND9vzCmbavYZMKK+tMvFqer7Y5bnBnPcdD4zSNH9C8lX11xuc82jMcMuT73yeZiqq37L6AqmSN9XRp1lescgc2/axrt9RtjtuLnkf2KHXOpar4rP2ownJPCur06qn/2z8ra4mxOxzhdVvTYMq5jjpnZ4vzy3mnvV1+/GqTOt4b23Rlt2bki15ss7kt9ukw7+6ciBQa71KK1X/YOeoffC4bjL32cdq5D7a4dy356v+eanqXqs/jQHjux9lKgculT60S+uda8E1y6eW729S06IGe951BZmvixG51fD8Fi339Kh+nv1/UtupmXYZme6TZxIeAZrqrrKG7HA+AwAATFnBp4pU+/NV2vVkVX8HmK68vOG9v5/uVqOvVe6/f0tN1zvz+kd7AABMiosxhXa2qfXBHoWnxxUf+ojG7AZte7VF3tFuxdZjZckeT8092xpNJ5dZrSWG/v+zv03Fd3VKd2/RW389WiufKxMtJwAAwEeIS5+ckWd+tNqvEYelS/n4wHoEJgBgMkXVs+QLqt7p0fqDB3UwvUPR0EHtWu2TToWdvmxGYQWSk/ftHL9nH+1U9bwiFS3uNkeeEpe/p8v861bTQgITIyE4AQAAAACYRCcV2hOXYhFFTkUHdR4ci4TU+0ZAmj7yUKhTUbC3XaEhLUPi+9vV9mJc7kXr1PBZZyYGITgBAAAAAJhEXlU94JHrQGeyz4j+Pic8Vge7tWrfU6CaxxtU+hFpTvDpglLpM3XatLZG1kAZsQMdql/UJde9m/S90YZNvcJdQcGJf1Ks6nqdqVqvUUegAQAAU9LMuU1qKnfrGmc6o+lu+e5ryv6wegBwBfEs2qE9nS2qKkt1Emt1OulRxZ2rtMW/S+u+lPXxRC6b/NtWacO8kFYusIIwJardHFPN5j3a8ZBP+aM9bniFm9IdYp7vuU+/2yRd/Z2N+sSnnJmZ/Pof9G9ffUQJfXF86wMAAAAAgA/FldNy4lN/qv/Sc0gzeghMAAAAAACQSy5vcOLAep2puk+/+XVUv2m2HqGwXta0s9xitWAw82MHpHMdqXXs6clgta5I7SP5av4HnXeW9RtPupPSl1uv4Y+IDNtfxz85S1JSj5ekL0/NG2mfAAAAAAB8tE1Ky4nE9uekB61WCj/R1UU/1YUnRwgIvLleFz5vrXNIrkrp4sOT0xfEx6o2Jvdhp8WZmcHo6bYCCDfrgnurs71D+s+NJxRPD1AcWO88ZmIvn9GzVdN2/fmQAMUfKS+1LDkd1m+ad+vq5DxadQAAAAAArjyTEJz4qf6j4F6nkp2v/+T7onTsH/X/hrQIuKhblDfX/jz98/Xm3y26MEmtJ8ZnjHQf2G3S/EVdvfCPnBnSx278n7rKpPv9Hmf02rnNQwIMf6SrK81bODw8OJOy6x+lB5s13ZkEAAAAAOBKMwnBiS9q2o1jd3897fMDlfzcMHq6z588Yf79qS58Ne2RjGQHm1Iiciq5jmXoYx3xXc6CTCq/RmsJAABy0Xtdqi0qUrGvWtUL7VeZt0hF3rL+6eoFJSoqmq+Oo853AADAJblyOsT8gP4j8lPzb71cziMdg15NdqDFHj3kp5q2dmCZ9cgKAACYeiL/t1d5a1/TQf8O7dhuXi8s15+ck1x3fcuetl47t6nlOq/cs50vAQCAS0JwYpzsR09O6OKgx1OiOp82nQpgXO08rgIAAKaqqALhCrUsTBt3/0RIfvNWc6PHnrZM+6TyPlWgmalB+wEAwCXJ7eCEM7JHToxiMfcWTbMe60jrJPN8zyr97qsDI438XsEXzb9pfWeY9L8/1mMdAAAgey5GFdrpVyjuTKecfl0ht09uu/fqpMihgCLyyVPocuZYzio+261PO1MAAODS5HZw4l9OJPt0sPp6SPxL8sMIhvQDYb3Shgsd6APiZl04ZmYce0S/c9ab2PCl1igbP9HVGvh+amSOVMee1sgg9sgjTjqeNPPW1tv77E9TatjQP9dFa9IazcPZ3r+lOtYEAAAfiujLK1V9/1L1Du0zYmaVWm5PazWhuMKhgDTbK88sZ1aSWzUrqjR2b1uTL/x8vd0Pxm1lKi4qUtt+ZwEAAFPAVQnD+QwAAHDlOOfXmrJGdZ2TGl44ppYbnPkjCqrdW6uO27fo2EOlzrzcEvtFQKFfh9TV0KZeVWnDW+tVweMmAIApgj4nAADAFSiu4LNrkoEJSzgyRutFq78Js26VN62/icsosi+gSLJJ5aXL+0ypSq8vUPKhE6uTTgITAIAphOAEAAC48pzoVtvmPrmmO9MXnPcMoof8Cskjb+Fk1PijCmzuVKDPmfwA4gcC6jHvrps8ctuzAACYEghOAACAK0xMvc+2KXbfOrXMs+cEIiftDyOK60jQL033yTPHmZWjwket8USkmklq4QEAwGQhOAEAAK4o8f0dattTo5bFFSqY7cx7334fWVihN8zbrV7ldpU/rGBvxLxXqXRu+ogiAADkPoITAAAgp8V/0aO2pjKVlZao2Neojn1R6WJEPatrVekz8+9Yo95TzspjuRhW95Nd8j6+XL7pLuV93Jl/IqKMvU6cCilgtl/qcdv9OeSqWFjBw+b9Rq/yDnWo0eRNSWmxypo6FGBAMABAjiM4AQAAclZ8f7vqvxVTVdte7Q28pbceman2e+5R9cKlCi3YpF0vPKL/EevS0r/pyRxcSBPd2aa2vFVafovdd8TMa52RNy7E7fd+EXUvqbaH5lzUpoCZE3x2qT29OWivkmNS/U3oULs6Dpdq/Z69emvvz/TY73eo/u52BYceIgAAOYTgBAAAyE0XQ+pcG1PT43XyOP1Quj5pfQgrdG2Dmm7MU/CFpeo+YWZNd+ma5BqjOOfXxtURNa2oUcE0e9bHpjltId6OaHCvEwWq+dYO7dhuXq++pWPHjumg35le5HXWyS2p/ibci7doy9e8yrOOcVqePnd9qXSiQ10/iSWXAwCQiwhOAACAnBQPdKu7oka+1IgaRvTdcPLdd9PnZYUpPAtWqe7OJm1aUZGcHk3o+Tb5b1+lhs86M4z8AmdMi3NTvVlBRKG9dn8Ty/985J4xeo/ZeQcAQC4iOAEAAHKSq2yVXrsvvaId15FDVusAj3ze/OQc13V1WrV6uXxOx5YZne5W+7c9avl66eB+I6523hVWZDL7ZTjQoZKiIhWN+CpT6xt+tfpGWma/SjaO8ShJLKTAPvN+nVfuIVGa4z+3W1QAAJDLrkoYzmcAAIDcdTGotnm16lSTtgWXa/wPV8TlX12ixhdHax3h0zr/JtXMciY/VFF1L14prf0A+9/fpuK7OqX7tungsvScCatrYaXWHJaqnnlL6780VvsSAACyg5YTAABgangvpMA58z7BIT3jBzrU/qt12nvsWLLviEEv/zr5kmsFFDmd/DAlWY+7WKGXGu+QnDkdlN8awUMV8t1IYAIAkLsITgAAgNx0yq+2JfVas9t+3iIa9Ctk3n3Xf27QoxmhjZVq25ehVYQ1dOjjAdU8UCX7QZAhZrll9zoRV/xi8sMUVqCZMwcPdhp72y/roY6CRQ2qmGHPAwAgFxGcAAAAOSim3qcb1bk7oK4D1jgaMb35hjMaxR+mhRnO+dXdW6WqeYMr5Smx3R3qmteimjnOjFGE353MTicm1ydmFjif0lwMqevZXmlOg9Z/wzu4rw0AAHIMwQkAAJC75tZp011exfZ1qLPPm+xnIpLquTIWVMeidrkebpDHGRo0Xexwl5of6lHpH49WMZ+pghvsT/GL5+0PU5Drxio1zYmo++VA8vEOXTSfV9SrXTXasLlF3rQRTwAAyEV0iAkAAHLTqV6tWdGqnveuUcEtf6n1D9Vo5s871bysXcH4J3SN93/rr1YsV9Vn0kMPcQWertX9z4cUtfqncOTdukGvPlkx8GhHzK81X2lTz+GwYs6spBlueb+yTlu+9mG2NLgMHWJaogF1PHy/OoLvm7S75Vu0XH/1lVLljxC4AQAg1xCcAAAAyKrLFJwAAGAKIzgBAACQZdGjIanQQysHAMAVi+AEAAAAAADIKjrEBAAAAAAAWUVwAgAAAAAAZBXBCQAAAAAAkFUEJwAAAAAAQFYRnAAAAAAAAFlFcAIAAAAAAGQVwQkAAAAAAJBVBCcAAAAAAEBWEZwAAAAAAABZRXACAAAAAABkFcEJAAAAAACQVQQnAAAAAABAVhGcAAAAAAAAWUVwAgAAAAAAZBXBCQAAAAAAkFUEJwAAAAAAQFYRnAAAAAAAAFlFcAIAAAAAAGQVwQkAAAAAAJBVBCcAAAAAAEBWEZwAAAAAAABZRXACAAAAAABkFcEJAAAAAACQVQQnAAAAAABAVhGcAAAAAAAAWUVwAgAAAAAAZBXBCQAAAAAAkFUEJwAAAAAAQFYRnAAAAAAAAFlFcAIAAAAAAGQVwQkAAAAAAJBVBCcAAAAAAEBWEZwAAAAAAABZRXACAAAAAABkFcEJAAAAAACQVQQnAAAAAABAVhGcAAAAAAAAWUVwAgAAAAAAZBXBCQAAAAAAkFUEJwAAAAAAQFYRnAAAAAAAAFlFcAIAAAAAAGQVwQkAAAAAAJBVBCcAAAAAAEBWEZwAAAAAAABZRXACAAAAAABkFcEJAAAAAACQVVf95t9/l3A+AwAAAAAAfOhoOQEAAAAAALKK4AQAAAAAAMgqghMAAAAAACCLpP8P4mLx9ke332kAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TuxxEFIKGv-5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.layers(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUarabp6DloW"
      },
      "source": [
        "## Agente\n",
        "Partes de nuestro agente:\n",
        "\n",
        "\n",
        "1.   Política: nuestra política fue una politica Epsilon greedy porque queremos explorar diferentes escenarios y no sólo explotar el mayor valor q.\n",
        "\n",
        "\n",
        "```\n",
        "def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(action_dim)  # Explorar\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state).float().view(-1)\n",
        "                q_values = self.q_network(state)\n",
        "            return np.argmax(q_values.numpy())  # Explotar\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "2.  Función Objetivo: En aprendizaje por refuerzo nuestra función objetico es maximizar la acción que sigue. Utilizamos la fórmula básica de TD learning: V(s) ← V(s) + α * [R + γ * V(s') - V(s)]\n",
        "\n",
        "```\n",
        "# Esto es la diferencia cuadrada entre la estimación y el valor de la acción.\n",
        "\n",
        "loss = ((q_values[action] - target) ** 2).mean()\n",
        "\n",
        "target = reward + 0.99 * next_q_values * (1 - int(done))\n",
        "\n",
        "```\n",
        "el 0.99 es nuestra gamma y de ahi checamos cual seria el valor futuro que se puede tener en el siguiente estado.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SppiQpqoLbWD",
        "outputId": "8b6fe350-334f-4d8d-aeb0-e38e6c18151c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_dim, action_dim, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.q_network = QNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        self.q_network.load_state_dict(torch.load(model_path))\n",
        "        self.q_network.eval()  \n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(action_dim)  # Explorar\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state).float().view(1, -1)\n",
        "                q_values = self.q_network(state)\n",
        "            return np.argmax(q_values.numpy())  # Explota\n",
        "        \n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_end:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        else:\n",
        "            self.epsilon = self.epsilon_end  \n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "      self.optimizer.zero_grad()\n",
        "      state = torch.tensor(state).float().view(1, -1)  \n",
        "      q_values = self.q_network(state)\n",
        "      with torch.no_grad():\n",
        "          next_state = torch.tensor(next_state).float().view(1, -1)  \n",
        "          next_q_values = self.q_network(next_state)\n",
        "      target = reward + 0.99 * next_q_values * (1 - int(done))\n",
        "\n",
        "      action = torch.tensor([action]).unsqueeze(0)  \n",
        "      q_values = q_values.gather(1, action)  \n",
        "\n",
        "      loss = ((q_values - target) ** 2).mean()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRp1Sm2lGbEg",
        "outputId": "51ed4d37-6b7c-4284-ed07-302e092546f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New max reward: 1043.0, model saved.\n",
            "0\n",
            "New max reward: 2789.0, model saved.\n",
            "New max reward: 3315.0, model saved.\n",
            "New max reward: 4479.0, model saved.\n",
            "New max reward: 4805.0, model saved.\n",
            "New max reward: 6274.0, model saved.\n",
            "New max reward: 9967.0, model saved.\n",
            "New max reward: 14977.0, model saved.\n"
          ]
        }
      ],
      "source": [
        "episodes = 2000000\n",
        "state_dim = 100800\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "agent = Agent(state_dim, action_dim)\n",
        "#agent.load_model(\"q_network_best.pth\")\n",
        "consecutive_wins = 0\n",
        "max_reward=0\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, _= env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done) \n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "    if total_reward>max_reward:\n",
        "      max_reward=total_reward\n",
        "      torch.save(agent.q_network.state_dict(), f\"q_network_best.pth\")  \n",
        "      print(f\"New max reward: {max_reward}, model saved.\")\n",
        "\n",
        "    if total_reward > 40000:  \n",
        "        consecutive_wins += 1\n",
        "        if consecutive_wins >= 2:  \n",
        "            print(\"Achieved back to back wins!\")\n",
        "            break\n",
        "    else:\n",
        "        consecutive_wins = 0  \n",
        "    if episode % 10000 == 0:\n",
        "        torch.save(agent.q_network.state_dict(), f\"q_network_{episode}.pth\")\n",
        "        print(episode)\n",
        "    agent.decay_epsilon()\n",
        "    #print(\"Episode: {}, Score: {}, Consecutive wins: {}\".format(episode,total_reward, consecutive_wins))\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0d2XW9F4kRr"
      },
      "outputs": [],
      "source": [
        "episodes = 2000000\n",
        "state_dim = 100800\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "agent = Agent(state_dim, action_dim)\n",
        "agent.load_model(\"q_network_best (1).pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSjQjLLfLk6D"
      },
      "outputs": [],
      "source": [
        "episodes = 20\n",
        "state_dim = 100800\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "consecutive_wins = 0\n",
        "max_reward=0\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, _,__= env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done) # Train the agent with the experience\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "    agent.decay_epsilon()\n",
        "    print(\"Episode: {}, Score: {}, Consecutive wins: {}\".format(episode,total_reward, consecutive_wins))\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
